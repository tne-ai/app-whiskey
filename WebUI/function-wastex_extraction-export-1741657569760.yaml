- id: wastex_extraction
  user_id: 8e57a1e6-aa7f-467c-8097-75c7e4f948d3
  name: wastex_extraction
  type: pipe
  content: "\"\"\"\ntitle: Example Filter\nauthor: open-webui\nauthor_url: https://github.com/open-webui\nfunding_url: https://github.com/open-webui\nversion: 0.1\n\"\"\"\n\n\"\"\"\ntitle: Example Filter\nauthor: open-webui\nauthor_url: https://github.com/open-webui\nfunding_url: https://github.com/open-webui\nversion: 0.1\n\"\"\"\n\nfrom pydantic import BaseModel, Field, ValidationError\nfrom typing import List, Literal, Union\nfrom fastapi import Request\n\nimport google.generativeai as genai\nfrom google.generativeai.types import GenerationConfig\n\nfrom open_webui.models.users import Users\nfrom open_webui.utils.chat import generate_chat_completion\n\nimport base64\nimport io\nimport os\n\n# Pydantic models for confidence details and material classification\n\n\nclass Confidence(BaseModel):\n    score: float = Field(..., description=\"Confidence score between 0 and 1\")\n    reasoning: str = Field(..., description=\"Explanation of the confidence score\")\n    assumptions: List[str] = Field(\n        ..., description=\"List of assumptions made during extraction\"\n    )\n\n\nclass MaterialClassification(BaseModel):\n    material: Literal[\n        \"Timber\",\n        \"Plastics\",\n        \"Plasterboard\",\n        \"Other Waste\",\n        \"Metals\",\n        \"Glass\",\n        \"Carpet\",\n        \"Concrete or Masonry\",\n        \"not applicable\",\n    ] = Field(\n        ...,\n        description=\"Material type, must be one of the allowed enumerations or 'not applicable'\",\n    )\n    sub_material: Literal[\n        \"MDF\",\n        \"Timber\",\n        \"Treated\",\n        \"Untreated\",\n        \"Weatherboard\",\n        \"Polystyrene\",\n        \"Plastic - Hard\",\n        \"Shrink Wrap (Pallets)\",\n        \"Building Wrap\",\n        \"HDPE\",\n        \"Polyethene\",\n        \"LDPE\",\n        \"Plasterboard\",\n        \"Linoleum\",\n        \"Cardboard\",\n        \"Non-Ferrous\",\n        \"Steel\",\n        \"Metals (mixed) e.g. metal joinery, fittings\",\n        \"Copper (pure)\",\n        \"Cable (copper)\",\n        \"Brass\",\n        \"Aluminium\",\n        \"Glass\",\n        \"Broadloom Carpet\",\n        \"Carpet Tiles\",\n        \"Underlay\",\n        \"Tiles\",\n        \"Fibre Cement (Cladding)\",\n        \"Concrete-based\",\n        \"Clay-based\",\n        \"Ceramic\",\n        \"Rubble\",\n        \"Concrete\",\n        \"not applicable\",\n    ] = Field(\n        ...,\n        description=\"Sub-material type, must be one of the allowed enumerations or 'not applicable'\",\n    )\n    confidence: Confidence = Field(\n        ..., description=\"Confidence details for material classification\"\n    )\n\n\n# Model for an ordered item (material classification extraction)\n\n\nclass OrderedItem(BaseModel):\n    raw_data: str = Field(..., description=\"Original CSV row data\")\n    material_classification: MaterialClassification = Field(\n        ..., description=\"Extracted material classification and confidence details\"\n    )\n\n\nclass MaterialsExtraction(BaseModel):\n    ordered_items: List[OrderedItem] = Field(\n        ...,\n        description=\"List of ordered items with raw data and extracted material classification\",\n    )\n\n\n# Pydantic models for volume/mass extraction\n\n\nclass VolumeMassData(BaseModel):\n    total_volume_m3: Union[float, str] = Field(\n        ...,\n        description=\"Total volume in cubic meters (m³) or 'Insufficient Data' if not computable\",\n    )\n    purchase_weight_kg: Union[float, str] = Field(\n        ...,\n        description=\"Total purchase weight in kilograms (kg) or 'Insufficient Data' if not computable\",\n    )\n    weight_per_unit: Union[float, str] = Field(\n        ...,\n        description=\"Weight per unit in kilograms (kg per unit) or 'Insufficient Data' if not available\",\n    )\n    calculation_method: str = Field(\n        ...,\n        description=\"Description of the general method used for volume calculation (e.g., 'rectangular', 'cylindrical', 'area with assumed thickness')\",\n    )\n    mass_calculation_method: str = Field(\n        ...,\n        description=\"Description of how the weight was derived (e.g., 'direct_weight', 'calculated_from_volume')\",\n    )\n    conversion_steps: List[str] = Field(\n        ...,\n        description=\"List of strings detailing every unit conversion step performed\",\n    )\n    confidence: Confidence = Field(\n        ..., description=\"Confidence details for volume/mass classification\"\n    )\n\n\nclass OrderedItem2(BaseModel):\n    raw_data: str = Field(..., description=\"Original CSV row data\")\n    volume_mass_data: VolumeMassData = Field(\n        ..., description=\"Extracted volume and mass information\"\n    )\n\n\nclass VolumeMassExtraction(BaseModel):\n    ordered_items: List[OrderedItem2] = Field(\n        ...,\n        description=\"List of ordered items with raw data and extracted volume/mass data\",\n    )\n\n\n# Combo model to combine material and volume/mass extraction results\n\n\nclass CombinedItem(BaseModel):\n    raw_data: str\n    volume_mass_data: VolumeMassData\n    material_classification: MaterialClassification\n\n\nclass ComboExtraction(BaseModel):\n    ordered_items: List[CombinedItem]\n\n\n# Pydantic models for final order extraction\n\n\nclass FinalOrder(BaseModel):\n    project_id: str = Field(..., description=\"Project ID, or 'Insufficient Data'\")\n    delivery_date: str = Field(\n        ..., description=\"Delivery date in ISO format or 'Insufficient Data'\"\n    )\n    stage: str = Field(..., description=\"Stage of the project or 'Insufficient Data'\")\n    trade_provider: str = Field(\n        ..., description=\"Trade provider or 'Insufficient Data'\"\n    )\n    item_name: str = Field(..., description=\"Item name or 'Insufficient Data'\")\n    material: str = Field(\n        ..., description=\"Material type (from material_classification)\"\n    )\n    sub_material: str = Field(\n        ...,\n        description=\"Sub-material type (from material_classification) or 'Insufficient Data'\",\n    )\n    excess_percentage: Union[float, str] = Field(\n        ..., description=\"Excess percentage or 'Insufficient Data'\"\n    )\n    density: Union[float, str] = Field(\n        ..., description=\"Density or 'Insufficient Data'\"\n    )\n    cubic_m3: Union[float, str] = Field(\n        ..., description=\"Volume in cubic meters (m³) or 'Insufficient Data'\"\n    )\n    weight_per_unit: Union[float, str] = Field(\n        ..., description=\"Weight per unit (kg) or 'Insufficient Data'\"\n    )\n    total_material_weight: Union[float, str] = Field(\n        ..., description=\"Total material weight (kg) or 'Insufficient Data'\"\n    )\n    waste_weight: Union[float, str] = Field(\n        ..., description=\"Waste weight or 'Insufficient Data'\"\n    )\n    waste_value: Union[float, str] = Field(\n        ..., description=\"Waste value or 'Insufficient Data'\"\n    )\n    unit_quantities: str = Field(\n        ..., description=\"Unit quantities or 'Insufficient Data'\"\n    )\n    unit_measure: str = Field(..., description=\"Unit measure or 'Insufficient Data'\")\n    price_per_unit: Union[float, str] = Field(\n        ..., description=\"Price per unit or 'Insufficient Data'\"\n    )\n    purchase_cost_total: Union[float, str] = Field(\n        ..., description=\"Total purchase cost or 'Insufficient Data'\"\n    )\n    estimated_removal_cost: Union[float, str] = Field(\n        ..., description=\"Estimated removal cost or 'Insufficient Data'\"\n    )\n    estimated_destination: str = Field(\n        ..., description=\"Estimated destination or 'Insufficient Data'\"\n    )\n    created_by_name: str = Field(\n        ..., description=\"Creator's name or 'Insufficient Data'\"\n    )\n    created_by_email: str = Field(\n        ..., description=\"Creator's email or 'Insufficient Data'\"\n    )\n\n\nclass VolMassMetadata(BaseModel):\n    calculation_method: str\n    mass_calculation_method: str\n    conversion_steps: List[str]\n    confidence: Confidence\n\n\nclass ExtendedFinalOrder(FinalOrder):\n    materials_metadata: Confidence\n    vol_mass_metadata: VolMassMetadata\n\n\nclass FinalOrderExtraction(BaseModel):\n    ordered_items: List[FinalOrder] = Field(\n        ..., description=\"List of final order objects with complete attributes\"\n    )\n\n\nclass ExtendedFinalOrderExtraction(BaseModel):\n    ordered_items: List[ExtendedFinalOrder]\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        MODEL_ID: str = Field(default=\"\")\n        GOOGLE_API_KEY: str = Field(default=\"\")\n\n    def __init__(self):\n        self.valves = self.Valves(\n            **{\n                \"GOOGLE_API_KEY\": os.getenv(\"GOOGLE_API_KEY\", \"\"),\n            }\n        )\n\n    async def pipe(self, body: dict, __user__: dict, __request__: Request) -> str:\n        # Retrieve user details using the provided user id.\n        user = Users.get_user_by_id(__user__[\"id\"])\n\n        file_body = body[\"messages\"][-1].get(\"file\", None)\n        file_content = body[\"messages\"][-1][\"content\"]\n        file_type = \"\"\n        if not file_body:\n            file_type = \"text/csv\"\n        else:\n            file_type = file_body[\"mime_type\"]\n\n        body[\"stream\"] = False\n\n        # ---------------------------------------------------------------------\n        # System prompt for CSV cleaning (data reorganization)\n        # ---------------------------------------------------------------------\n        csv_cleaning_prompt = \"\"\"\n        You are an expert in **data cleaning and CSV reorganization**.\n\n        1. **Input Sources**  \n           - You will receive a **messy** CSV file that may contain numerous blank rows, extraneous or repeated header lines, inconsistent formatting, merged cells, or other irregularities.\n        \n        2. **Output Requirements**  \n           - Transform the messy CSV into a **single, well-structured** CSV table.  \n           - **Retain all original data** from the input, ensuring no information is lost.  \n           - Organize the data logically into rows and columns so it can be readily understood and utilized.  \n        \n        3. **Transformation Rules**  \n           - Identify and merge any fragmented rows or columns into **cohesive** rows and columns.  \n           - Remove superfluous blank lines or repeated headers while preserving **all unique content**.  \n           - If the CSV contains multiple implied headers or partial row entries, unify them into a single, coherent set of column headers if possible.  \n           - Ensure that each row in the final output corresponds to a single logical record.  \n           - If certain data is unclear or cannot be placed definitively, place it in a separate column or note it in a way that preserves the information.\n           - ENSURE you include the column names in the first row of the table.\n           - When copying values that have embedded commas, ENSURE you wrap the value with quotation marks in your CSV output\n        \n        \n        4. **Output Details**  \n           - **Output only** the cleaned CSV, with no additional footers, explanatory text, or formatting.  \n           - DO NOT INCLUDE ANY 'csv' HEADER ON THE FIRST ROW - OUTPUT ONLY THE CSV CONTENT (COLUMN NAMES MUST BE ON THE FIRST ROW)\n           - The final CSV should have clear, consistent columns and rows that reflect **all** data from the original input.\n        \n        5. **Validation & Step-by-Step Reasoning**  \n             1. Parse the CSV and identify meaningful column headers, data rows, and any stray text.  \n             2. Organize rows and columns so they match logically and no data is lost.  \n             3. Ensure every piece of information from the messy CSV has a place in the final output.  \n           - Verify you have not discarded or overwritten any data.  \n           - Confirm the structure is cohesive, with each record on its own row and each field in its correct column.\n\n        Your response should contain ONLY the CSV data. Do not include any backticks (\"`\"), code fences, comments, etc.\n        THE FIRST LINE OF YOUR OUTPUT MUST BE THE ROW OF COLUMN NAMES. IT IS IMPERATIVE THAT THE FIRST LINE OF YOUR OUTPUT IS THE ROW OF COLUMN NAMES!\n        \"\"\"\n\n        # For image files, use the image extraction prompt.\n        image_extraction_prompt = \"\"\"\n        1. Input:\n           - The provided file (PDF or image) may contain one or more tables with various layouts, borders, and fonts. Tables might span multiple pages or have complex structures such as merged cells.\n        \n        2. Extraction Requirements:\n           - Identify and extract every **product line item** present in the file that contains line items of an order.\n             - Exclude or ignore any rows that do not represent actual line items (e.g., shipping or handling lines, extra charges, and other rows with no valid products in them).\n           - Unify all **actual line-item** information into a single, coherent, CSV representation. Your output must be one SINGLE CSV table.\n           - Preserve the original rows and columns for those line items as accurately as possible.\n           - If a table has merged or nested cells, flatten the structure into a coherent CSV representation.\n           - Ensure that all numerical and textual data for line items is captured without omission.\n        \n        3. Formatting:\n           - Output only valid CSV data with the first row containing the column headers.\n           - Do not include any additional text, explanations, or formatting in your output.\n           - When data contains embedded commas, enclose the values in quotation marks to maintain CSV integrity.\n        \n        4. Validation:\n           - Verify that each **actual line item** is fully and accurately extracted.\n           - Confirm that the CSV output reflects the original table structures (for line items) without any loss of data.\n        \n        Your response should contain ONLY the CSV data. Do not include any backticks (\"`\"), code fences, comments, etc.\n        THE FIRST LINE OF YOUR OUTPUT MUST BE THE ROW OF COLUMN NAMES. IT IS IMPERATIVE THAT THE FIRST LINE OF YOUR OUTPUT IS THE ROW OF COLUMN NAMES!\n        \"\"\"\n\n        # ---------------------------------------------------------------------\n        # Helper function: Calls the chat completion API with the current body,\n        # validates the response against the provided Pydantic schema, and retries\n        # until either a valid response is obtained or the max retries are reached.\n        # This function uses and updates the nonlocal 'retry_count' variable.\n        # ---------------------------------------------------------------------\n        async def get_validated_response(schema_model, prompt_desc: str) -> any:\n            nonlocal retry_count, max_retries, body, __request__, user, i\n            while retry_count < max_retries:\n                chat_call = await generate_chat_completion(__request__, body, user)\n                response_content = chat_call[\"choices\"][0][\"message\"][\"content\"]\n                try:\n                    validated_output = schema_model.model_validate_json(\n                        response_content\n                    )\n                    print(f\"Successfully processed {prompt_desc} batch {i}\")\n                    return validated_output\n                except ValidationError:\n                    retry_count += 1\n                    print(\"RESPONSE CONTENT: \\n\\n\" + response_content)\n                    print(\"Failed, trying again\")\n                    continue\n            return None\n\n        # Helper function for Google Gemini API calls\n        async def call_google_gemini(\n            system_prompt, image_content, image_type=\"image/jpeg\"\n        ):\n            # Configure Google API with the API key from environment or valves\n            # if not self.valves.GOOGLE_API_KEY:\n            #     return \"Error: GOOGLE_API_KEY is not set\"\n            google_api_key = \"<FILL IN API KEY>\"\n            genai.configure(api_key=google_api_key)\n            model_id = \"gemini-2.0-pro-exp\"\n\n            # Format content for Gemini API\n            contents = []\n\n            # Add system prompt as a user message (since Gemini doesn't have system role)\n            if system_prompt:\n                contents.append(\n                    {\"role\": \"user\", \"parts\": [{\"text\": f\"System: {system_prompt}\"}]}\n                )\n\n            # Handle the image content\n            parts = []\n\n            # For single image\n            if not isinstance(image_content, list):\n                image_data = image_content\n                if image_data.startswith(\"data:\"):\n                    image_data = image_data.split(\",\")[1]\n\n                parts.append(\n                    {\"inline_data\": {\"mime_type\": image_type, \"data\": image_data}}\n                )\n            else:\n                # For multiple images (PDF case)\n                for image_item in image_content:\n                    if image_item.get(\"type\") == \"image_url\":\n                        image_url = image_item[\"image_url\"][\"url\"]\n                        # If it's a data URL, extract the base64 part\n                        if image_url.startswith(\"data:\"):\n                            image_data = image_url.split(\",\")[1]\n                        else:\n                            image_data = image_url\n\n                        parts.append(\n                            {\n                                \"inline_data\": {\n                                    \"mime_type\": \"image/jpeg\",\n                                    \"data\": image_data,\n                                }\n                            }\n                        )\n\n            # Add the parts to the contents\n            contents.append({\"role\": \"user\", \"parts\": parts})\n\n            # Create model and configuration\n            model = genai.GenerativeModel(model_name=model_id)\n            generation_config = GenerationConfig(\n                temperature=0.7,\n                top_p=0.9,\n                top_k=40,\n                max_output_tokens=16384,\n            )\n\n            try:\n                response = model.generate_content(\n                    contents,\n                    generation_config=generation_config,\n                )\n                return response.text\n            except Exception as e:\n                return f\"Error calling Google Gemini API: {str(e)}\"\n\n        # ---------------------------------------------------------------------\n        # First model call: Branch based on file type.\n        # ---------------------------------------------------------------------\n        if file_type == \"text/csv\":\n            # For CSV files, use the original approach\n            body[\"model\"] = \"gpt-4o\"\n            body[\"messages\"][0][\"content\"] = csv_cleaning_prompt\n\n            # Call the completion function to obtain the cleaned CSV output.\n            first_call = await generate_chat_completion(__request__, body, user)\n            first_response = first_call[\"choices\"][0][\"message\"][\"content\"]\n\n        elif file_type == \"image/png\" or file_type == \"image/jpeg\":\n            # For image files, use Google Gemini\n            print(\"Processing image with Google Gemini\")\n            first_response = await call_google_gemini(\n                image_extraction_prompt, file_content, file_type\n            )\n\n        else:\n            # For PDF files (which are a list of images)\n            print(\"Processing PDF images with Google Gemini\")\n            first_response = await call_google_gemini(\n                image_extraction_prompt, file_content\n            )\n        print(\"RECEIVED:\", first_response)\n\n        body[\"model\"] = \"gpt-4o\"\n\n        # Split the CSV output into rows and separate the header from data rows.\n        rows = first_response.splitlines()\n        if not rows:\n            return \"error in parsing rows\"  # or handle error accordingly\n        header_row = rows[0]\n        print(\"HEADER ROW: \\n\\n\", header_row)\n        data_rows = rows[1:]\n\n        # ---------------------------------------------------------------------\n        # Define system prompts for the three extraction steps.\n        # ---------------------------------------------------------------------\n        materials_prompt = \"\"\"\n        You are an expert construction materials analyst. Your task is to analyze construction material orders and extract standardized information about material type and quantity.\n        \n        For each line item in the document:\n        Identify the material and sub-material from the allowed enumerations. Pay attention to descriptive terms, brand names, and common abbreviations that indicate material types.\n        Transfer the original CSV row data into the output under the 'raw_data' field.\n        Assign confidence scores (0-1) for material classification:\n        Material Classification Confidence:\n        1.0: Exact match to enumerated material/sub-material\n        0.9: Standard industry abbreviation/brand name\n        0.8: Clear inference from product type\n        0.6: Ambiguous between similar sub-materials\n        0.4: Only main material category clear\n        0.2: Material inferred from context only\n        \n        Materials and submaterials:\n        It's also important to identify the material of a line item. You must use only the pairings of material and submaterial shown below:\n        \n        Timber,MDF\n        Timber,Timber\n        Timber,Treated\n        Timber,Untreated\n        Timber,Weatherboard\n        Plastics,Polystyrene\n        Plastics,Plastic - Hard\n        Plastics,Shrink Wrap (Pallets)\n        Plastics,Building Wrap\n        Plastics,HDPE\n        Plastics,Polyethene\n        Plastics,LDPE\n        Plasterboard,Plasterboard\n        Other Waste,Linoleum\n        Other Waste,Cardboard\n        Metals,Non-Ferrous\n        Metals,Steel\n        Metals,\"Metals (mixed) e.g. metal joinery, fittings\"\n        Metals,Copper (pure)\n        Metals,Cable (copper)\n        Metals,Brass\n        Metals,Aluminium\n        Glass,Glass\n        Carpet,Broadloom Carpet\n        Carpet,Carpet Tiles\n        Carpet,Underlay\n        Concrete or Masonry,Tiles\n        Concrete or Masonry,Fibre Cement (Cladding)\n        Concrete or Masonry,Concrete-based\n        Concrete or Masonry,Clay-based\n        Concrete or Masonry,Ceramic\n        Concrete or Masonry,Rubble\n        Concrete or Masonry,Concrete\n\n        Important: If a line item does not pertain to material (for example, if it is not an actual item or contains no material-related information), assign \"not applicable\" to the material, sub_material, and the reasoning and assumptions fields within Confidence instead of forcing a classification. Assign the confidence score to be 0.\n        \n        Your response should conform to the following schema:\n        \n        {\n          \"ordered_items\": [{\n            \"raw_data\": \"string\",\n            \"material_classification\": {\n              \"material\": \"string (one of [Timber, Plastics, Plasterboard, Other Waste, Metals, Glass, Carpet, Concrete or Masonry, not applicable])\",\n              \"sub_material\": \"string (one of [MDF, Timber, Treated, Untreated, Weatherboard, Polystyrene, Plastic - Hard, Shrink Wrap (Pallets), Building Wrap, HDPE, Polyethene, LDPE, Plasterboard, Linoleum, Cardboard, Non-Ferrous, Steel, Metals (mixed) e.g. metal joinery, fittings, Copper (pure), Cable (copper), Brass, Aluminium, Glass, Broadloom Carpet, Carpet Tiles, Underlay, Tiles, Fibre Cement (Cladding), Concrete-based, Clay-based, Ceramic, Rubble, Concrete, not applicable])\",\n              \"confidence\": {\n                \"score\": \"float\",\n                \"reasoning\": \"string\",\n                \"assumptions\": [\"string\"]\n              }\n            }\n          }]\n        }\n        \n        Very important: Respond with ONLY JSON conforming to the schema. Do not include any backticks (\"`\"), code fences, comments, etc. Only respond with conforming JSON.\n        \"\"\"\n\n        volume_mass_prompt = \"\"\"\n        You are an expert in material quantity calculations, dimensional analysis, unit conversions, and weight extraction. Your task is to analyze a set of CSV rows representing a construction material order and extract mass (weight) data and/or standardized volume. \n        The input is provided as a list of objects, with each objection containing the raw csv data for that line item as well as a material classification. For each line item, perform the following steps and be as explicit as possible in showing all your calculation steps, assumptions, and conversion details:\n        \n        1. First, check if there is direct weight information in the raw_data (e.g., \"500kg\", \"10KG\", \"2.5KG\"). If available, record the purchase weight and, if present, the weight per unit. Document the exact source of these numbers.\n\n        2. If direct weight data is not available, extract any available dimensional information from the input row. This may include explicit dimensions (e.g., \"100x45x200 mm\", \"6M rebar\", \"Diameter 12mm, Length 6M\") or implicit measurements embedded in the text. Clearly show all intermediate values. \n           - If multiple dimension sets appear in a single item description (e.g., \"100 X 50 (90X45)\"), prefer to use the dimensions inside the parentheses (\"90X45\") for your calculations.\n    \n        3. Determine the appropriate calculation method for volume and weight estimation:\n             • For items with three dimensions, use the rectangular prism formula: Volume = Length × Width × Height (ensuring all dimensions are converted to meters).\n             • For cylindrical items, use the formula: Volume = π × (Diameter/2)² × Length (with proper unit conversions).\n             • If only an area is provided, assume a reasonable thickness based on industry standards and document this assumption.\n           Note: These examples are provided for guidance. Think through your calculation approach based on the specific data and show every step. Use your knowledge of volume formulas to choose the correct approach given the item under consideration.\n    \n        4. Calculate the total volume in cubic meters (m³). If applicable, convert provided dimensions (e.g., from mm or cm to m) and document all unit conversion steps explicitly in a \"conversion_steps\" field. If the provided data is ambiguous or incomplete, output \"Insufficient Data\" for total volume.\n    \n        5. Assign a confidence score (between 0 and 1) based on the clarity of the provided dimensions, unit conversions, and any assumptions made. Clearly document your reasoning, list every assumption, and include explicit conversion steps.\n    \n        6. DO NOT infer density or weight if it is not explicitly provided. Only return weight values if they are directly given in the data. If no direct weight is provided, mark the weight fields as \"Insufficient Data.\"\n    \n        7. Output your result as a JSON object with exactly one field \"ordered_items\", which is a list of objects. \n    \n        Your response should conform to the following schema:\n    \n        {\n          \"ordered_items\": [{\n            \"raw_data\": \"string\",\n            \"volume_mass_data\": {\n              \"total_volume_m3\": \"number or 'Insufficient Data'\",\n              \"purchase_weight_kg\": \"number or 'Insufficient Data'\",\n              \"weight_per_unit\": \"number or 'Insufficient Data'\",\n              \"calculation_method\": \"string\",\n              \"mass_calculation_method\": \"string\",\n              \"conversion_steps\": [\"string\"],\n              \"confidence\": {\n                \"score\": \"float\",\n                \"reasoning\": \"string\",\n                \"assumptions\": [\"string\"]\n              }\n            }\n          }]\n        }\n    \n        Example (demonstrating correct approach to parentheses dimensions):\n    \n        If the item name is \"100 X 50 (90X45) Rad Nst Ut Pg Kd 28 @ 6.000 RANDOM\" and the quantity is 168, and you see \"@ 6.000\" indicating a 6-meter length, then:\n          - Extract the 90mm x 45mm as the cross-section (ignore the \"100 X 50\" outside the parentheses).\n          - Convert 90mm to 0.09m and 45mm to 0.045m, length 6.000m to 6.0m.\n          - Apply rectangular prism formula: Volume per piece = 0.09m × 0.045m × 6.0m = 0.0243m³.\n          - Multiply by quantity 168 to get total volume = 168 × 0.0243m³ = 4.0824m³.\n          - If no weight is explicitly mentioned, \"purchase_weight_kg\" and \"weight_per_unit\" both become \"Insufficient Data.\"\n    \n        Very important: Respond with ONLY JSON conforming to the schema. Do not include any backticks (\"`\"), code fences, comments, etc. Only respond with conforming JSON.\n        \"\"\"\n\n        final_order_prompt = \"\"\"\n        You are an expert in final order data extraction and mapping. Your task is to take as input a ComboExtraction – a JSON object with a single field \"ordered_items\", where each item contains:\n          • raw_data: the original CSV row data,\n          • material_classification: an object with fields \"material\", \"sub_material\", and associated confidence details,\n          • volume_mass_data: an object with fields \"total_volume_m3\", \"purchase_weight_kg\", \"weight_per_unit\", \"calculation_method\", \"mass_calculation_method\", \"conversion_steps\", and confidence details.\n        \n        Using the information from each ComboExtraction item, map the available data into a final order object with the following attributes:\n          - project_id: string (if not available, use \"Insufficient Data\")\n          - delivery_date: string (delivery date in ISO format or \"Insufficient Data\")\n          - stage: string (or \"Insufficient Data\")\n          - trade_provider: string (or \"Insufficient Data\")\n          - item_name: string\n          - material: string (taken from material_classification.material)\n          - sub_material: string (taken from material_classification.sub_material)\n          - excess_percentage: number or \"Insufficient Data\"\n          - density: number or \"Insufficient Data\"\n          - cubic_m3: number (from volume_mass_data.total_volume_m3) or \"Insufficient Data\"\n          - weight_per_unit: number (from volume_mass_data.weight_per_unit) or \"Insufficient Data\"\n          - total_material_weight: number (from volume_mass_data.purchase_weight_kg) or \"Insufficient Data\"\n          - waste_weight: number or \"Insufficient Data\"\n          - waste_value: number or \"Insufficient Data\"\n          - unit_quantities: string (or \"Insufficient Data\")\n          - unit_measure: string (or \"Insufficient Data\")\n          - price_per_unit: number or \"Insufficient Data\"\n          - purchase_cost_total: number or \"Insufficient Data\"\n          - estimated_removal_cost: number or \"Insufficient Data\"\n          - estimated_destination: string (or \"Insufficient Data\")\n          - created_by_name: string (or \"Insufficient Data\")\n          - created_by_email: string (or \"Insufficient Data\")\n        \n        Important:\n          • Map only the data that is present; do not force or invent values for any attribute. \n          • Ensure you fill in something meaningful for 'item_name' - do your best to deduce a reasonable item name from the 'raw_data' field\n          • If an attribute is missing or cannot be reliably derived from the input ComboExtraction, set its value to \"Insufficient Data\".\n          • Your output must be a JSON object with a single field \"ordered_items\", which is a list of final order objects exactly matching the schema below.\n          • Do not include any extra text, code fences, or formatting. Respond with ONLY valid JSON.\n        \n        Your response should conform to the following schema:\n        \n        {\n          \"ordered_items\": [{\n            \"project_id\": \"string\",\n            \"delivery_date\": \"string\",\n            \"stage\": \"string\",\n            \"trade_provider\": \"string\",\n            \"item_name\": \"string\",\n            \"material\": \"string (from material_classification.material)\",\n            \"sub_material\": \"string (from material_classification.sub_material or 'Insufficient Data')\",\n            \"excess_percentage\": \"number or 'Insufficient Data'\",\n            \"density\": \"number or 'Insufficient Data'\",\n            \"cubic_m3\": \"number or 'Insufficient Data'\",\n            \"weight_per_unit\": \"number or 'Insufficient Data'\",\n            \"total_material_weight\": \"number or 'Insufficient Data'\",\n            \"waste_weight\": \"number or 'Insufficient Data'\",\n            \"waste_value\": \"number or 'Insufficient Data'\",\n            \"unit_quantities\": \"string or 'Insufficient Data'\",\n            \"unit_measure\": \"string or 'Insufficient Data'\",\n            \"price_per_unit\": \"number or 'Insufficient Data'\",\n            \"purchase_cost_total\": \"number or 'Insufficient Data'\",\n            \"estimated_removal_cost\": \"number or 'Insufficient Data'\",\n            \"estimated_destination\": \"string or 'Insufficient Data'\",\n            \"created_by_name\": \"string or 'Insufficient Data'\",\n            \"created_by_email\": \"string or 'Insufficient Data'\"\n          }]\n        }\n\n        Very important: Respond with ONLY JSON conforming to the schema. Do not include any backticks (\"`\"), code fences, comments, etc. Only respond with conforming JSON.\n        \"\"\"\n\n        all_extracted_final_items = []\n        batch_size = 20  # Process rows in batches\n        max_retries = 3  # Maximum number of retries for each API call\n        print(\"Total rows: \" + str(len(data_rows)))\n        print(\"Batch size: \" + str(batch_size))\n        # Process each batch of CSV data rows\n        for i in range(0, len(data_rows), batch_size):\n            # Reset retry counter at the beginning of each batch\n            retry_count = 0\n\n            # Create a batch that includes the header row and a subset of data rows.\n            current_batch = [header_row] + data_rows[i : i + batch_size]\n            print(f\"Processing batch {i}\")\n\n            # ---------------------------\n            # Step 1: Material Classification Extraction\n            # ---------------------------\n            body[\"messages\"] = [\n                {\"role\": \"system\", \"content\": materials_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Here is the table data: \\n\" + \"\\n\".join(current_batch),\n                },\n            ]\n            materials_output = await get_validated_response(\n                MaterialsExtraction, \"materials\"\n            )\n\n            if not materials_output:\n                print(f\"Unsuccessfully processed batch {i}\")\n                return f\"Unsuccessfully processed batch {i}\"\n\n            print(\"Materials Output: \\n\\n\", materials_output.model_dump_json(indent=4))\n            materials_metadata = [\n                mat.material_classification.confidence\n                for mat in materials_output.ordered_items\n            ]\n            # ---------------------------\n            # Step 2: Volume/Mass Extraction\n            # ---------------------------\n            body[\"messages\"] = [\n                {\"role\": \"system\", \"content\": volume_mass_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Here are the column names (associated with the 'raw_data' attribute): \\n\"\n                    + header_row\n                    + \"\\n\\nHere is the data: \\n\"\n                    + materials_output.model_dump_json(indent=4),\n                },\n            ]\n            body[\"model\"] = \"o3-mini\"\n            # body[\"model\"] = \"gpt-4o\"\n            vol_mass_output = await get_validated_response(\n                VolumeMassExtraction, \"vol/mass\"\n            )\n\n            if not vol_mass_output:\n                print(f\"Unsuccessfully processed batch {i}\")\n                return f\"Unsuccessfully processed batch {i}\"\n\n            print(\"Vol/Mass output: \\n\\n\", vol_mass_output.model_dump_json(indent=4))\n            vol_mass_metadata = [\n                VolMassMetadata(\n                    calculation_method=vm.volume_mass_data.calculation_method,\n                    mass_calculation_method=vm.volume_mass_data.mass_calculation_method,\n                    conversion_steps=vm.volume_mass_data.conversion_steps,\n                    confidence=vm.volume_mass_data.confidence,\n                )\n                for vm in vol_mass_output.ordered_items\n            ]\n\n            # ---------------------------\n            # Step 3: Combine Extraction Results\n            # ---------------------------\n            try:\n                combined_items = [\n                    CombinedItem(\n                        raw_data=mat_item.raw_data,\n                        volume_mass_data=vol_item.volume_mass_data,\n                        material_classification=mat_item.material_classification,\n                    )\n                    for mat_item, vol_item in zip(\n                        materials_output.ordered_items, vol_mass_output.ordered_items\n                    )\n                ]\n            except (ValidationError, AttributeError, TypeError, IndexError) as e:\n                print(f\"Error during combination: {e}\")\n                return f\"Error during combination: {e}\"\n            except Exception as e:\n                print(f\"Unexpected error: {e}\")\n                return f\"Unexpected error: {e}\"\n\n            combo_extraction = ComboExtraction(ordered_items=combined_items)\n\n            # ---------------------------\n            # Step 4: Final Order Extraction\n            # ---------------------------\n            body[\"model\"] = \"gpt-4o\"\n            body[\"messages\"] = [\n                {\"role\": \"system\", \"content\": final_order_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Here are the column names (associated with the 'raw_data' attribute): \\n\"\n                    + header_row\n                    + \"\\n\\nHere is the data: \\n\"\n                    + combo_extraction.model_dump_json(indent=4),\n                },\n            ]\n            final_order_output = await get_validated_response(\n                FinalOrderExtraction, \"final order\"\n            )\n            if not final_order_output:\n                print(f\"Unsuccessfully processed batch {i}\")\n                return f\"Unsuccessfully processed batch {i}\"\n\n            extended_final_order_output = ExtendedFinalOrderExtraction(\n                ordered_items=[\n                    ExtendedFinalOrder(\n                        **final_order.dict(),\n                        materials_metadata=materials_metadata[idx],\n                        vol_mass_metadata=vol_mass_metadata[idx],\n                    )\n                    for idx, final_order in enumerate(final_order_output.ordered_items)\n                ]\n            )\n            # Append the extracted final order items to the complete list.\n            all_extracted_final_items.extend(extended_final_order_output.ordered_items)\n\n        # Combine all final order items into the final extraction JSON.\n        final_extraction = ExtendedFinalOrderExtraction(\n            ordered_items=all_extracted_final_items\n        )\n        print(\"FINAL FINAL: \\n\\n\" + final_extraction.model_dump_json(indent=4))\n        return final_extraction.model_dump_json(indent=4)\n"
  meta:
    description: convert messy csv to desired format
    manifest:
      title: Example Filter
      author: open-webui
      author_url: https://github.com/open-webui
      funding_url: https://github.com/open-webui
      version: "0.1"
  is_active: true
  is_global: false
  updated_at: 1741657559
  created_at: 1740970767
